{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "- **x1** - source → c → sink\n",
    "- **x2** - source → c ← sink\n",
    "- **x3** - source ← c → sink\n",
    "- **x4** - source ← c ← sink\n",
    "- **x5** - common nodes (#c)\n",
    "- **x6** - Number following (Source).\n",
    "- **x7** - Number of followers (Sink)\n",
    "- **x8** - Adamic Adar\n",
    "- **x9** - Jaccard's coeffcient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       x1  x2  x3  x4  x5  y      x6   x7        x8        x9\n",
      "0      59   0  57   0  60  1  101514   71  5.823572  0.000590\n",
      "1       2   2   2   2   2  1     222   36  0.199040  0.001986\n",
      "2      19   0  11   0  19  1   18915   40  2.325911  0.000998\n",
      "3       1   0   1   0   1  1  764195    2  0.092985  0.000001\n",
      "4       0   0   0   0   0  1    4435    1  0.000000  0.000000\n",
      "...    ..  ..  ..  ..  .. ..     ...  ...       ...       ...\n",
      "19996  23   0  23   0  23  1   15948   40  2.105039  0.001440\n",
      "19997  69   0  69   0  69  1   35111  142  7.528091  0.001960\n",
      "19998   0   0   0   0   0  1    1579    9  0.000000  0.000000\n",
      "19999  35   0  34   0  35  1  137761   43  3.891917  0.000254\n",
      "20000   0   0   0   0   0  1  422106    1  0.000000  0.000000\n",
      "\n",
      "[20001 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "pos_trainData = pd.read_csv('Total_Features/Pos_Total_Feature.csv', sep=',')\n",
    "print(pos_trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       x1  x2  x3  x4  x5    x6   x7  y        x8        x9\n",
      "0       1   0   1   0   1  2722   11  0  0.088650  0.000366\n",
      "1       0   0   0   0   0   743    1  0  0.000000  0.000000\n",
      "2       0   0   0   0   0    79    4  0  0.000000  0.000000\n",
      "3       0   0   0   0   0     9    1  0  0.000000  0.000000\n",
      "4       0   0   0   0   0   142    1  0  0.000000  0.000000\n",
      "...    ..  ..  ..  ..  ..   ...  ... ..       ...       ...\n",
      "19996   0   0   0   0   0    39    1  0  0.000000  0.000000\n",
      "19997   0   0   0   0   0    48    1  0  0.000000  0.000000\n",
      "19998  14   0   9   0  15   584  155  0  1.566421  0.020380\n",
      "19999   0   0   0   0   0     1    3  0  0.000000  0.000000\n",
      "20000   0   0   0   0   0   507    3  0  0.000000  0.000000\n",
      "\n",
      "[20001 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "neg_trainData = pd.read_csv('Total_Features/Neg_Total_Feature.csv', sep=',')\n",
    "print(neg_trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       x1  x2  x3  x4  x5  y      x6   x7        x8        x9\n",
      "0      59   0  57   0  60  1  101514   71  5.823572  0.000590\n",
      "1       2   2   2   2   2  1     222   36  0.199040  0.001986\n",
      "2      19   0  11   0  19  1   18915   40  2.325911  0.000998\n",
      "3       1   0   1   0   1  1  764195    2  0.092985  0.000001\n",
      "4       0   0   0   0   0  1    4435    1  0.000000  0.000000\n",
      "...    ..  ..  ..  ..  .. ..     ...  ...       ...       ...\n",
      "39997   0   0   0   0   0  0      39    1  0.000000  0.000000\n",
      "39998   0   0   0   0   0  0      48    1  0.000000  0.000000\n",
      "39999  14   0   9   0  15  0     584  155  1.566421  0.020380\n",
      "40000   0   0   0   0   0  0       1    3  0.000000  0.000000\n",
      "40001   0   0   0   0   0  0     507    3  0.000000  0.000000\n",
      "\n",
      "[40002 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "trainData = pd.concat([pos_trainData, neg_trainData], ignore_index=True)\n",
    "print(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       x1  x2  x3  x4  x5  y      x6  x7        x8        x9\n",
      "0       0   0   0   0   0  1  280325   4  0.000000  0.000000\n",
      "1       5   0   4   0   5  1   25437  10  0.447018  0.000196\n",
      "2       1   0   1   0   1  0     466   7  0.077271  0.002020\n",
      "3      18   0  18   0  18  1   14222  68  1.760742  0.001261\n",
      "4       0   0   0   0   0  0      73   1  0.000000  0.000000\n",
      "...    ..  ..  ..  ..  .. ..     ...  ..       ...       ...\n",
      "39997   6   0   6   0   6  1    4819   9  0.579288  0.001236\n",
      "39998  41   0  45   0  45  1  105865  52  4.256149  0.000424\n",
      "39999   0   0   0   0   0  0      13   1  0.000000  0.000000\n",
      "40000   0   0   0   0   0  0      32   5  0.000000  0.000000\n",
      "40001   1   0   1   0   1  1     675  11  0.087795  0.001325\n",
      "\n",
      "[40002 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "trainData = shuffle(trainData).reset_index(drop=True)\n",
    "print(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_AUC (X_train, X_test, y_train, y_test, clf) :\n",
    "    try :\n",
    "        predict_prob = clf.predict_proba(X_test)[:,1:]\n",
    "        y_pred_train = clf.predict(X_train)[:,1:]\n",
    "        y_pred_test = clf.predict(X_test)[:,1:]\n",
    "    except:\n",
    "        predict_prob = clf.predict(X_test)\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "    \n",
    "    auc = metrics.roc_auc_score(y_test, predict_prob)\n",
    "    \n",
    "    w_sklearn = np.r_[clf.intercept_, clf.coef_.squeeze()]\n",
    "    print(\"Weights: {}\".format(w_sklearn))\n",
    "    print(\"AUC: {}\".format(auc))\n",
    "    print('Train MSE:', mean_squared_error(y_pred_train, y_train))\n",
    "    print('Test MSE:', mean_squared_error(y_pred_test, y_test))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_pred - y_true)**2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        1\n",
      "1        1\n",
      "2        0\n",
      "3        1\n",
      "4        0\n",
      "        ..\n",
      "39997    1\n",
      "39998    1\n",
      "39999    0\n",
      "40000    0\n",
      "40001    1\n",
      "Name: y, Length: 40002, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = trainData['y']\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_all = trainData[['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9']]\n",
    "X_common_JC = trainData[['x1', 'x2', 'x3', 'x4', 'x9']]\n",
    "X = X_common_JC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 32001 instances. Test set has 8001 instances.\n",
      "\n",
      "------Logistic Regression------\n",
      "Weights: [-1.18798437  0.50212076  0.13913571  0.25756467 -0.09490479 -1.59847264]\n",
      "AUC: 0.835324542093227\n",
      "Train MSE: 0.16643229899065654\n",
      "Test MSE: 0.16297962754655668\n",
      "\n",
      "-------Linear Regression-------\n",
      "Weights: [ 4.09632118e-01  4.95223291e-03 -1.25773577e-04  1.17503717e-03\n",
      " -2.45896774e-03  5.32456842e+00]\n",
      "AUC: 0.8608734509123265\n",
      "Train MSE: 0.20652458898487233\n",
      "Test MSE: 0.2053489223822633\n",
      "\n",
      "--------Ridge Regression-------\n",
      "Weights: [ 4.10750147e-01  4.86166151e-03 -1.23789853e-04  1.30570709e-03\n",
      " -2.37921853e-03  4.79123743e+00]\n",
      "AUC: 0.8614924423056901\n",
      "Train MSE: 0.20656451454783836\n",
      "Test MSE: 0.20540987593536547\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=90051)\n",
    "print(\"Training set has {} instances. Test set has {} instances.\".format(X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "print('\\n------Logistic Regression------')\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "calculate_AUC(X_train, X_test, y_train, y_test, model)\n",
    "\n",
    "print('\\n-------Linear Regression-------')\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "calculate_AUC(X_train, X_test, y_train, y_test, lr)\n",
    "\n",
    "print('\\n--------Ridge Regression-------')\n",
    "rr = linear_model.Ridge(alpha=.5)\n",
    "rr.fit(X_train, y_train)\n",
    "calculate_AUC(X_train, X_test, y_train, y_test, rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      x1  x2  x3  x4  x5   x6  x7        x8        x9\n",
      "0      0   0   0   0   0   21  29  0.000000  0.000000\n",
      "1      0   0   0   0   0   71   9  0.000000  0.000000\n",
      "2      2   0   2   0   3  205  17  0.459422  0.011152\n",
      "3      2   0   2   0   2  506  36  0.178376  0.003670\n",
      "4      0   0   0   0   0   18  46  0.000000  0.000000\n",
      "...   ..  ..  ..  ..  ..  ...  ..       ...       ...\n",
      "1995   0   0   0   0   0   53   2  0.000000  0.000000\n",
      "1996   0   0   1   0   1   95  41  0.100383  0.006061\n",
      "1997   0   0   0   0   0   27   2  0.000000  0.000000\n",
      "1998   0   0   0   0   0   56   3  0.000000  0.000000\n",
      "1999   0   0   0   0   0  244   2  0.000000  0.000000\n",
      "\n",
      "[2000 rows x 9 columns]\n",
      "(2000, 9)\n",
      "range(0, 10)\n"
     ]
    }
   ],
   "source": [
    "testData = pd.read_csv('Total_Features/Test_Total_Feature.csv', sep=',')\n",
    "print(testData)\n",
    "print(testData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [-2.10022009e+00  2.23285461e-01  6.45215503e-02  1.58571763e-01\n",
      "  2.01590187e-02  2.42136202e-01  1.01933958e-04  7.64161509e-03\n",
      "  5.66715403e-02 -7.52930947e-04]\n"
     ]
    }
   ],
   "source": [
    "# model = LogisticRegression()\n",
    "# model.fit(X, y)\n",
    "\n",
    "# w_sklearn = np.r_[model.intercept_, model.coef_.squeeze()]\n",
    "# print(\"Weights: {}\".format(w_sklearn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         Id  Predicted\n",
       "0        1   0.132795\n",
       "1        2   0.116685\n",
       "2        3   0.393347\n",
       "3        4   0.373917\n",
       "4        5   0.148442\n",
       "...    ...        ...\n",
       "1995  1996   0.111102\n",
       "1996  1997   0.202485\n",
       "1997  1998   0.110840\n",
       "1998  1999   0.111889\n",
       "1999  2000   0.113039\n",
       "\n",
       "[2000 rows x 2 columns]>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_new = testData\n",
    "# y_new = model.predict_proba(X_new)[:,1:]\n",
    "\n",
    "# output = pd.DataFrame({'Id': testData.index+1, 'Predicted': y_new.flatten()})\n",
    "# output.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output.to_csv('output_v5-9-logreg.csv', sep=\",\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
